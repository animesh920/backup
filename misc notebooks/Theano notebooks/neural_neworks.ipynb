{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images', 'data', 'target_names', 'DESCR', 'target']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=data['data']\n",
    "y=data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recode_y(y):\n",
    "    final=np.zeros((y.shape[0],10))\n",
    "    for i in range(10):\n",
    "        final[:,i]=(y==i)+0\n",
    "    return final\n",
    "recode_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "np.random.seed(123)\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20)\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_ohe=recode_y(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(5.2208969482734435)]\n",
      "[array(4.8231805133849)]\n",
      "[array(4.509148421985462)]\n",
      "[array(4.248155856310939)]\n",
      "[array(4.028156193641526)]\n",
      "[array(3.8417159361910977)]\n",
      "[array(3.6820139044344233)]\n",
      "[array(3.5431104515184186)]\n",
      "[array(3.420501849533105)]\n",
      "[array(3.311087294916551)]\n",
      "[array(3.212806315798694)]\n",
      "[array(3.1242516710688486)]\n",
      "[array(3.044383140207211)]\n",
      "[array(2.972353726081241)]\n",
      "[array(2.907420089386911)]\n",
      "[array(2.848904625353148)]\n",
      "[array(2.7961838734302082)]\n",
      "[array(2.748687096346099)]\n",
      "[array(2.7058964256973637)]\n",
      "[array(2.667345126137365)]\n",
      "[array(2.6326135074263375)]\n",
      "[array(2.6013233709045425)]\n",
      "[array(2.573132189503291)]\n",
      "[array(2.547727971428409)]\n",
      "[array(2.524825306465852)]\n",
      "[array(2.50416266942517)]\n",
      "[array(2.485500767742389)]\n",
      "[array(2.4686215899042128)]\n",
      "[array(2.4533278056838963)]\n",
      "[array(2.439442237189989)]\n",
      "[array(2.4268072148728517)]\n",
      "[array(2.41528372330446)]\n",
      "[array(2.4047503120507687)]\n",
      "[array(2.3951017935593564)]\n",
      "[array(2.386247775714193)]\n",
      "[array(2.378111087135425)]\n",
      "[array(2.3706261539375646)]\n",
      "[array(2.363737381813247)]\n",
      "[array(2.3573975898740662)]\n",
      "[array(2.351566534365049)]\n",
      "[array(2.3462095520677213)]\n",
      "[array(2.3412963453048503)]\n",
      "[array(2.3367999230882512)]\n",
      "[array(2.3326957061214237)]\n",
      "[array(2.328960797075828)]\n",
      "[array(2.325573411824283)]\n",
      "[array(2.322512462208866)]\n",
      "[array(2.319757276559296)]\n",
      "[array(2.3172874407026516)]\n",
      "[array(2.315082739755613)]\n",
      "[array(2.313123179663219)]\n",
      "[array(2.3113890672754827)]\n",
      "[array(2.309861128678203)]\n",
      "[array(2.308520647377459)]\n",
      "[array(2.3073496065576005)]\n",
      "[array(2.306330822731417)]\n",
      "[array(2.305448061395432)]\n",
      "[array(2.3046861285344207)]\n",
      "[array(2.304030934769902)]\n",
      "[array(2.303469531459695)]\n",
      "[array(2.3029901200419594)]\n",
      "[array(2.3025820373511676)]\n",
      "[array(2.3022357205387847)]\n",
      "[array(2.301942655676582)]\n",
      "[array(2.3016953141866763)]\n",
      "[array(2.3014870810278425)]\n",
      "[array(2.301312178161444)]\n",
      "[array(2.3011655863042093)]\n",
      "[array(2.3010429674148294)]\n",
      "[array(2.300940589807189)]\n",
      "[array(2.300855257269065)]\n",
      "[array(2.3007842431120493)]\n",
      "[array(2.300725229697668)]\n",
      "[array(2.3006762536755074)]\n",
      "[array(2.300635656931499)]\n",
      "[array(2.3006020430675522)]\n",
      "[array(2.3005742391118598)]\n",
      "[array(2.300551262079369)]\n",
      "[array(2.3005322899606284)]\n",
      "[array(2.3005166366991476)]\n",
      "[array(2.3005037307215757)]\n",
      "[array(2.3004930966020214)]\n",
      "[array(2.300484339468979)]\n",
      "[array(2.3004771317934423)]\n",
      "[array(2.3004712022331604)]\n",
      "[array(2.300466326240764)]\n",
      "[array(2.3004623181787487)]\n",
      "[array(2.300459024715947)]\n",
      "[array(2.300456319308689)]\n",
      "[array(2.300454097598646)]\n",
      "[array(2.3004522735808206)]\n",
      "[array(2.300450776418968)]\n",
      "[array(2.300449547802344)]\n",
      "[array(2.3004485397546444)]\n",
      "[array(2.3004477128202034)]\n",
      "[array(2.3004470345639505)]\n",
      "[array(2.300446478331708)]\n",
      "[array(2.3004460222274354)]\n",
      "[array(2.300445648268926)]\n",
      "[array(2.3004453416923076)]\n"
     ]
    }
   ],
   "source": [
    "#creating a container for the data storage variable\n",
    "\n",
    "def floatX(x):\n",
    "    ''' theano is very particular about the data type of the shared variable espescially the data type of the \n",
    "    shared variable is supposed to be theano.config.floatX\n",
    "    \n",
    "    this function converts the input array in a theano compatiable array\n",
    "    '''\n",
    "    return np.asarray(x,dtype=theano.config.floatX)\n",
    "\n",
    "input_dim=64\n",
    "hidden_dim=625\n",
    "output_dim=10\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "X=T.fmatrix()\n",
    "Y=T.fmatrix()\n",
    "\n",
    "#creating the initial weight variables\n",
    "\n",
    "# W1=theano.shared(np.random.rand(input_dim,hidden_dim))\n",
    "# b1=floatX(theano.shared(np.zeros(hidden_dim)))\n",
    "\n",
    "# W2=floatX(theano.shared(np.random.rand(hidden_dim,output_dim)))\n",
    "# b2=floatX(theano.shared(np.zeros(output_dim)))\n",
    "\n",
    "\n",
    "\n",
    "def floatX(x):\n",
    "    return np.asarray(x,dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.rand(*shape)))\n",
    "\n",
    "def init_bias(shape):\n",
    "    return theano.shared(np.asarray(np.zeros(shape),dtype=theano.config.floatX))\n",
    "\n",
    "\n",
    "W1=init_weights((64,100))\n",
    "b1=init_bias(100)\n",
    "\n",
    "W2=init_weights((100,10))\n",
    "b2=init_bias(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model(X,W1,b1,W2,b2):\n",
    "    a1=T.dot(X,W1)+b1\n",
    "    z1=T.nnet.sigmoid(a1)\n",
    "    a2=T.dot(z1,W2)+b2\n",
    "    z2=T.nnet.softmax(a2)\n",
    "    return z2\n",
    "\n",
    "\n",
    "py_x=model(X,W1,b1,W2,b2)\n",
    "\n",
    "nn_pred=T.argmax(py_x,axis=1)\n",
    "\n",
    "cost=T.mean(T.nnet.categorical_crossentropy(py_x,Y))\n",
    "\n",
    "dW1=T.grad(cost,W1)\n",
    "db1=T.grad(cost,b1)\n",
    "\n",
    "dW2=T.grad(cost,W2)\n",
    "db2=T.grad(cost,b2)\n",
    "\n",
    "updates=[[W1,W1-learning_rate*dW1],[b1,b1-learning_rate*db1],[W2,W2-learning_rate*dW2],[b2,b2-learning_rate*db2]]\n",
    "\n",
    "# updates=[[W1,W1-learning_rate*dW1],[W2,W2-learning_rate*dW2]]\n",
    "\n",
    "\n",
    "train=theano.function(inputs=[X,Y],outputs=[cost],updates=updates,allow_input_downcast=True)\n",
    "\n",
    "prediction=theano.function(inputs=[X],outputs=[nn_pred],allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    cost=train(x_train,y_train_ohe)\n",
    "    print cost\n",
    "    predicted_output=prediction(x_val)\n",
    "    predicted_output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX=x_train\n",
    "trY=y_train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_dim=64\n",
    "hidden_dim=625\n",
    "output_dim=10\n",
    "\n",
    "\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "def sgd(cost, params, lr=0.05):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        updates.append([p, p - g * lr])\n",
    "    return updates\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return theano.tensor.switch(x<0, 0, x)\n",
    "\n",
    "def model(X, w_h, w_o):\n",
    "#     h = T.nnet.sigmoid(T.dot(X, w_h))\n",
    "    h = relu(T.dot(X, w_h))\n",
    "    pyx = T.nnet.softmax(T.dot(h, w_o))\n",
    "    return pyx\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((input_dim, hidden_dim))\n",
    "w_o = init_weights((hidden_dim, output_dim))\n",
    "\n",
    "py_x = model(X, w_h, w_o)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "params = [w_h, w_o]\n",
    "updates = sgd(cost, params)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "best_param={}\n",
    "best_accuracy=0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n",
    "        cost = train(trX[start:end], trY[start:end])\n",
    "\n",
    "    #print np.mean(np.argmax(teY, axis=1) == predict(teX))\n",
    "#     print cost\n",
    "#     print predict(x_val)\n",
    "    accuracy=np.mean(predict(x_val)==y_val)\n",
    "    if best_accuracy<accuracy:\n",
    "        best_accuracy=accuracy\n",
    "        best_param[\"W1\"]=W1.get_value()\n",
    "        best_param[\"W2\"]=W2.get_value()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best accuracy on training this network,I got is of 96%. Now furhter tuning in terms of learning rate and the number of iteration can be done. But i am not doing it right now.\n",
    "\n",
    "For the same network archetecture but now using a reLu unit as the hidden activation function i get 96.7% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
