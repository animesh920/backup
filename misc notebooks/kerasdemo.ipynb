{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Demo!\n",
    "Lets import some stuff! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "print keras.__version__ # this should be 1.0.0\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "#sanity check to make sure everything works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flies = ['airplane']\n",
    "sits = ['bench','chair','sofa','stool','toilet']\n",
    "water = ['cup','bathtub','flower_pot','toilet','vase','bottle','bowl']\n",
    "batch_size = 64\n",
    "box_size = 32\n",
    "\n",
    "classes = [sits, water,flies]\n",
    "nb_classes = len(classes)\n",
    "patch_size = box_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shallow_model():\n",
    "\tnb_filters = 32\n",
    "\tnb_pool = 2\n",
    "\tnb_conv = [5,3]\n",
    "    #this starts a new empty model\n",
    "\tmodel = Sequential() #types are sequential or graph. I used sequential.\n",
    "    #add a convolutional layer\n",
    "\tmodel.add(Convolution3D(nb_filters, kernel_dim1=nb_conv[0], kernel_dim2=nb_conv[0], \n",
    "\t\tkernel_dim3=nb_conv[0], border_mode='valid',\n",
    "\t\t\tinput_shape=(1, box_size, box_size, box_size),init='he_normal',\n",
    "\t\t\tsubsample=(2,2,2)\n",
    "\t\t\t))\n",
    "    #this is an activation layer\n",
    "\tmodel.add(LeakyReLU(alpha=.1))\n",
    "    #Dropout helps prevent overfitting\n",
    "\tmodel.add(Dropout(.2))\n",
    "    #a second convolutional layer\n",
    "\tmodel.add(Convolution3D(nb_filters, kernel_dim1=nb_conv[1], kernel_dim2=nb_conv[1], \n",
    "\t\tkernel_dim3=nb_conv[1], border_mode='valid',init='he_normal'))\n",
    "\n",
    "\tmodel.add(LeakyReLU(alpha=.1))\n",
    "    #Some max pooling, to reduce size and help generalize\n",
    "\tmodel.add(MaxPooling3D())\n",
    "\tmodel.add(Dropout(.3))\n",
    "\t\n",
    "\tmodel.add(Flatten())\n",
    "    #dense layer\n",
    "\tmodel.add(Dense(128,init='uniform'))\n",
    "\tmodel.add(Dropout(.4))\n",
    "    #a final dense layer\n",
    "\tmodel.add(Dense(nb_classes,init='uniform'))\n",
    "    #sigmoid will output 3 probabilities which dont have to add up to 3\n",
    "\tmodel.add(Activation('sigmoid'))\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = shallow_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "dot = model_to_dot(model,show_shapes=True)\n",
    "SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok lets compile our model    \n",
    "this might take a while (or it might not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "sgd = SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "optimizer = sgd\n",
    "start = time.time()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "print 'model compiled in ' + str(time.time() - start) + ' seconds'\n",
    "weights_file=\"shallow_weights_bench_039.h5\"\n",
    "model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the model weights and biases. This lets us know the model has loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.layers[0].get_weights()[0].shape #the weights\n",
    "print model.layers[0].get_weights()[1].shape #the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing the model\n",
    "If we want to test this on some actual data, we need a few other things to help us. this loads input data from a file and returns it piece by piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import npytar\n",
    "cfg = {'batch_size' : 32,\n",
    "\t   'reg' : 0.001,\n",
    "\t   'momentum' : 0.9,\n",
    "\t   'dims' : (32, 32, 32),\n",
    "\t   'n_channels' : 1,\n",
    "\t   'n_classes' : 10,\n",
    "\t   'batches_per_chunk': 2,\n",
    "\t   'max_epochs' : 80,\n",
    "\t   'max_jitter_ij' : 2,\n",
    "\t   'max_jitter_k' : 2,\n",
    "\t   'n_rotations' : 12,\n",
    "\t   'checkpoint_every_nth' : 4000,\n",
    "\t   }\n",
    "\n",
    "size = 2316\n",
    "size -= size%64\n",
    "\n",
    "def data_loader(fname):\n",
    "    dims = cfg['dims']\n",
    "    chunk_size = cfg['batch_size']*cfg['batches_per_chunk']\n",
    "    xc = np.zeros((chunk_size, cfg['n_channels'],)+dims, dtype=np.float32)\n",
    "    reader = npytar.NpyTarReader(fname)\n",
    "    yc = []\n",
    "    for ix, (x, name) in enumerate(reader):\n",
    "        cix = ix % chunk_size\n",
    "        xc[cix] = x.astype(np.float32)\n",
    "        feature = [1,0,0] #theyre all benches, this is their feature\n",
    "        yc.append(feature)\n",
    "        if len(yc) == chunk_size:\n",
    "            #print yc\n",
    "            yield (2.0*xc - 1.0, np.asarray(yc, dtype=np.float32))\n",
    "            yc = []\n",
    "            xc.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model, for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_file = \"left_out_bench_left_out.tar\"\n",
    "loader = (data_loader(test_file))\n",
    "progbar = generic_utils.Progbar(size)\n",
    "y_true = [] #truth\n",
    "y_pred = [] #predictions\n",
    "for X_batch,y_batch in loader:\n",
    "    pred = model.predict_on_batch(X_batch)\n",
    "    y_pred.append(pred)\n",
    "    y_true.append(y_batch)\n",
    "    progbar.add(X_batch.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did we do?\n",
    "We now have `y_pred` and `y_true` in memory.    \n",
    "Lets compare them and see how they match up\n",
    "\n",
    "These are predictions of what you can do with a bench, given that the model has never seen a bench before. Ideally, the model should say that a bench has a high sittable probability and a low probability elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reshape the data because of shape weirdness.\n",
    "fixed_true = np.vstack(y_true)\n",
    "fixed_pred = np.vstack(y_pred).reshape(-1,nb_classes)\n",
    "print fixed_true.shape, fixed_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_true = fixed_true.flatten()\n",
    "class_pred = fixed_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve,average_precision_score\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "precision, recall, thresholds = precision_recall_curve(class_true,class_pred)\n",
    "mAP = average_precision_score(class_true,class_pred)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(recall, precision, label='Bench')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall for Bench: AUC={0:0.2f}'.format(mAP))\n",
    "plt.legend(loc=\"lower left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For fun, lets find where precision * recall is largest\n",
    "\n",
    "then we can make a confusion matrix for this best case scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptimesr = precision*recall\n",
    "maxind = np.argmax(ptimesr)\n",
    "print maxind, ptimesr[maxind],precision[maxind],recall[maxind],thresholds[maxind]\n",
    "from sklearn.preprocessing import Binarizer\n",
    "#binarize the labels at this threshold\n",
    "binarizer = Binarizer(threshold=thresholds[maxind])\n",
    "binary_predictions = binarizer.fit_transform(class_pred.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(class_true,binary_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest',cmap = plt.cm.Blues)\n",
    "plt.title(\"Best Case Bench Confusion Matrix\")\n",
    "tick_marks = np.arange(2)\n",
    "for x in xrange(2):\n",
    "    for y in xrange(2):\n",
    "        plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                     color='y',size=24)\n",
    "plt.xticks(tick_marks, [\"Negative\",\"Positive\"], rotation=45)\n",
    "plt.yticks(tick_marks,[\"Negative\",\"Positive\"])\n",
    "plt.colorbar()\n",
    "plt.grid(b=False)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
