{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "data=scipy.io.loadmat('ex4data1.mat')\n",
    "weights=scipy.io.loadmat('ex4weights.mat')\n",
    "\n",
    "X=data['X']\n",
    "y=data['y']\n",
    "\n",
    "theta=[weights['Theta1'],weights['Theta2']]\n",
    "\n",
    "print theta[0].shape\n",
    "print theta[1].shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the function takes a list of neural network weights and propogates it forward to generate the output of the neural net\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "def feedforward(X,theta):\n",
    "    output=X\n",
    "    for i in range(len(theta)):\n",
    "        output=np.hstack((np.ones((5000,1)),output))\n",
    "        output=sigmoid(np.dot(output,theta[i].T))\n",
    "        #adding an extra bias term to the activations\n",
    "    #output=output[:,1:] #bias term is not added in the output for the final layer hence removing the bias term\n",
    "    return output\n",
    "\n",
    "def recode_y(y):\n",
    "    #convert y from 5000 x 1 to 10 x 5000\n",
    "    \n",
    "    temp=np.zeros((5000,10))\n",
    "    for i,elem in zip(range(len(y)),y):\n",
    "        temp[i,elem-1]=1\n",
    "    return temp.T\n",
    "    \n",
    "        \n",
    "y_new=recode_y(y)\n",
    "A=feedforward(X,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287629165161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' the correct output should be 0.287'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_theta(theta):\n",
    "    final=[]\n",
    "    for elem in theta:\n",
    "        elem=elem.reshape(-1)\n",
    "        final.append(elem)\n",
    "    final=[elem1 for elem in final for elem1 in elem]\n",
    "    final=np.asarray(final)\n",
    "    return final\n",
    "\n",
    "\n",
    "def unflatten_theta(theta):\n",
    "    theta_1=theta[:10025]\n",
    "    theta_1=theta_1.reshape((25,401))\n",
    "    theta_2=theta[10025:]\n",
    "    theta_2=theta_2.reshape((10,26))\n",
    "    \n",
    "    return [theta_1,theta_2]\n",
    "\n",
    "\n",
    "\n",
    "def nnCostFunction1(theta,X,y):\n",
    "    '''\n",
    "    X is 5000 x 400\n",
    "    y is 10 x 5000\n",
    "    theta is a list of weights\n",
    "    '''\n",
    "    \n",
    "    y=y.T\n",
    "    \n",
    "    num_layers=len(theta)+1\n",
    "    A=feedforward(X,theta)\n",
    "    B=y\n",
    "    m=y.shape[0]\n",
    "    cost=np.sum((np.sum(((-B*np.log(A))-((1-B)*np.log(1-A))),axis=1)),axis=0)\n",
    "    cost=cost/m\n",
    "    return cost\n",
    "\n",
    "print nnCostFunction1(theta,X,y_new)\n",
    "\n",
    "\n",
    "''' the correct output should be 0.287'''\n",
    "\n",
    "# temp=unflatten_theta(flatten_theta(theta))\n",
    "# print len(temp)\n",
    "# temp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOnce you are done, ex4.m will call your nnCostFunction using the loaded\\nset of parameters for Theta1 and Theta2, and \\x15 = 1. You should see that\\nthe cost is about 0.383770.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RegularizednnCostFunction(theta,X,y_new,lmbda):\n",
    "    \n",
    "#     theta_1=theta[:10025]\n",
    "#     theta_1=theta_1.reshape((25,401))\n",
    "    \n",
    "#     theta_2=theta[10025:]\n",
    "#     theta_2=theta_2.reshape((10,26))\n",
    "    \n",
    "#     theta=[theta_1,theta_2]\n",
    "    \n",
    "    \n",
    "    unregularized_cost=nnCostFunction(X,y,theta)\n",
    "    m=y.shape[0]\n",
    "    #removing the theta for bias term from the cost function, bias term theta is in the first column\n",
    "    theta_no_bias=[theta[0][:,1:],theta[1][:,1:]]\n",
    "    reg_term=0\n",
    "    for i in range(len(theta_no_bias)):\n",
    "        reg_term=reg_term+np.sum(np.sum(np.square(theta_no_bias[i]),axis=1),axis=0)\n",
    "    return (lmbda/(2.0*m))*reg_term+unregularized_cost\n",
    "\n",
    "#print RegularizednnCostFunction(X,y_new,theta,1)\n",
    "\n",
    "'''\n",
    "Once you are done, ex4.m will call your nnCostFunction using the loaded\n",
    "set of parameters for Theta1 and Theta2, and \u0015 = 1. You should see that\n",
    "the cost is about 0.383770.\n",
    "'''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    sig=sigmoid(z)\n",
    "    return (sig)*(1-sig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_initialization(nn_architecture):\n",
    "    num_layers=len(nn_architecture)\n",
    "    theta=[]\n",
    "    for prev_layer,next_layer in zip(nn_architecture[:num_layers-1],nn_architecture[1:num_layers]):\n",
    "        #+1 added because thera is a bias term in the prev layer hence a wt has to be assigned to that bias term\n",
    "        theta.append(np.random.rand(next_layer,prev_layer+1))\n",
    "    return theta\n",
    "        \n",
    "        \n",
    "temp_theta=random_initialization([400,25,10])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10285,)\n"
     ]
    }
   ],
   "source": [
    "def feedForward1(X,theta):\n",
    "    m,n=X.shape\n",
    "    one_rows = np.ones((1, np.shape(X)[0] ))\n",
    "    a1=np.hstack((np.ones((5000,1)),X)).T  #401 x 5000\n",
    "    z2 = theta[0].dot( a1 )\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.r_[one_rows, a2] \n",
    "    z3 = theta[1].dot( a2 )\n",
    "    a3 = sigmoid( z3 )\n",
    "    return a1,a2,a3,z2,z3\n",
    "                 \n",
    "\n",
    "def backPropogation(theta,X,y_new):\n",
    "    ''' assumes that the shape of X is 5000 x 400\n",
    "    the shape of y is 10 x 5000\n",
    "    theta is a list of weights\n",
    "    '''\n",
    "    m,n=X.shape\n",
    "    a1,a2,a3,z2,z3=feedForward1(X,theta)\n",
    "    \n",
    "    sigma3=(a3-y_new) # 10 x 5000\n",
    "    #sigma2 = theta[1].T.dot( sigma3 ) * sigmoidGradient( np.vstack((np.ones((1,5000)),z2)))\n",
    "    sigma2 = theta[1].T.dot( sigma3 ) * sigmoid_gradient( np.r_[np.ones((1, m)), z2 ] )\n",
    "    #sigma2 = theta[1].T.dot( sigma3 ) * a2*(1-a2)\n",
    "    sigma2=sigma2[1:,:]\n",
    "    \n",
    "    \n",
    "    print 'sigma3',sigma3.shape\n",
    "    print 'sigma2',sigma2.shape\n",
    "    \n",
    "    print 'a1',a1.shape\n",
    "    print 'a2',a2.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    accum1 = sigma2.dot( a1.T ) / m\n",
    "    accum2 = sigma3.dot( a2.T ) / m\n",
    "    \n",
    "    accum = np.array([accum1.T.reshape(-1).tolist() + accum2.T.reshape(-1).tolist()]).T\n",
    "    return np.ndarray.flatten(accum)\n",
    " \n",
    "\n",
    "bp=backPropogation(theta,X,y_new)\n",
    "\n",
    "print bp.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.18712766e-05,   9.38798109e-05,  -1.92593606e-04, ...,\n",
       "         1.27957448e-03,   1.35707295e-03,   7.73329872e-04])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "a,b=unflatten_theta(bp)\n",
    "print a.shape\n",
    "print b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_check(theta,X,y_new):\n",
    "    ''' X is 5000 x 400\n",
    "    y is 10 x 5000\n",
    "    theta is a list of weights\n",
    "    '''\n",
    "    epsilon=0.0001\n",
    "    m,n=X.shape\n",
    "    numerical_grads=np.empty(10285)\n",
    "    \n",
    "    flat_theta=flatten_theta(theta)\n",
    "    \n",
    "    for elem in range(10285):\n",
    "#         if elem%100 ==0:\n",
    "#             print elem\n",
    "        temp_theta=flat_theta\n",
    "        old_cost=nnCostFunction1(theta,X,y_new)\n",
    "        temp_theta[elem]=(temp_theta[elem]+epsilon)\n",
    "        new_cost=nnCostFunction1(unflatten_theta(temp_theta),X,y_new)\n",
    "        \n",
    "        grad=(new_cost-old_cost)/epsilon\n",
    "        \n",
    "        numerical_grads[elem]=grad\n",
    "    return numerical_grads\n",
    "\n",
    "#numerical_grad=grad_check(theta,X,y_new)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.53332271e-05   6.53332271e-05   6.53332271e-05   6.53373805e-05\n",
      "   6.52843840e-05   6.54266765e-05   6.70249335e-05   6.61358128e-05\n",
      "   6.46822330e-05   6.42738006e-05]\n",
      "[  6.18712766e-05   9.38798109e-05  -1.92593606e-04  -1.68494568e-04\n",
      "   3.48682758e-04   2.30505234e-04  -4.19602375e-05   1.64744107e-05\n",
      "   3.42491549e-04  -2.09724069e-05]\n"
     ]
    }
   ],
   "source": [
    "print numerical_grad[:10]\n",
    "print bp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0 94.6562662236\n",
      "cost after iteration 1 73.6015906656\n",
      "cost after iteration 2 52.5958217733\n",
      "cost after iteration 3 32.074297274\n",
      "cost after iteration 4 15.06295741\n",
      "cost after iteration 5 7.34496126363\n",
      "cost after iteration 6 5.37643466656\n",
      "cost after iteration 7 4.71805438208\n",
      "cost after iteration 8 4.42547467177\n",
      "cost after iteration 9 4.27389893459\n",
      "cost after iteration 10 4.18777403437\n",
      "cost after iteration 11 4.13561552093\n",
      "cost after iteration 12 4.10241981006\n",
      "cost after iteration 13 4.08037459899\n",
      "cost after iteration 14 4.06515348144\n",
      "cost after iteration 15 4.05425089559\n",
      "cost after iteration 16 4.04616602931\n",
      "cost after iteration 17 4.03997589245\n",
      "cost after iteration 18 4.03510051337\n",
      "cost after iteration 19 4.03116864803\n",
      "cost after iteration 20 4.02793856065\n",
      "cost after iteration 21 4.02525012476\n",
      "cost after iteration 22 4.02299528609\n",
      "cost after iteration 23 4.02109956013\n",
      "cost after iteration 24 4.0195102973\n",
      "cost after iteration 25 4.01818916736\n",
      "cost after iteration 26 4.01710730965\n",
      "cost after iteration 27 4.01624218567\n",
      "cost after iteration 28 4.01557552772\n",
      "cost after iteration 29 4.01509199764\n",
      "cost after iteration 30 4.01477830791\n",
      "cost after iteration 31 4.01462264449\n",
      "cost after iteration 32 4.01461428711\n",
      "cost after iteration 33 4.01474335876\n",
      "cost after iteration 34 4.01500065967\n",
      "cost after iteration 35 4.01537755652\n",
      "cost after iteration 36 4.01586590763\n",
      "cost after iteration 37 4.01645801148\n",
      "cost after iteration 38 4.01714657026\n",
      "cost after iteration 39 4.01792466326\n",
      "cost after iteration 40 4.0187857263\n",
      "cost after iteration 41 4.01972353516\n",
      "cost after iteration 42 4.02073219148\n",
      "cost after iteration 43 4.02180611012\n",
      "cost after iteration 44 4.02294000759\n",
      "cost after iteration 45 4.02412889095\n",
      "cost after iteration 46 4.02536804718\n",
      "cost after iteration 47 4.02665303274\n",
      "cost after iteration 48 4.02797966341\n",
      "cost after iteration 49 4.0293440042\n",
      "4.01461428711\n",
      "cost after iteration 0 94.6562662236\n",
      "cost after iteration 1 63.081979844\n",
      "cost after iteration 2 18.0980014267\n",
      "cost after iteration 3 5.73099336187\n",
      "cost after iteration 4 14.6363972163\n",
      "cost after iteration 5 28.1232292434\n",
      "cost after iteration 6 48.3534933053\n",
      "cost after iteration 7 78.6988893977\n",
      "cost after iteration 8 124.216983536\n",
      "cost after iteration 9 192.49412474\n",
      "cost after iteration 10 294.909836543\n",
      "cost after iteration 11 448.533404236\n",
      "cost after iteration 12 678.968755752\n",
      "cost after iteration 13 nan\n",
      "cost after iteration 14 nan\n",
      "cost after iteration 15 nan\n",
      "cost after iteration 16 nan\n",
      "cost after iteration 17 nan\n",
      "cost after iteration 18 nan\n",
      "cost after iteration 19 nan\n",
      "cost after iteration 20 nan\n",
      "cost after iteration 21 nan\n",
      "cost after iteration 22 nan\n",
      "cost after iteration 23 nan\n",
      "cost after iteration 24 nan\n",
      "cost after iteration 25 nan\n",
      "cost after iteration 26 nan\n",
      "cost after iteration 27 nan\n",
      "cost after iteration 28 nan\n",
      "cost after iteration 29 nan\n",
      "cost after iteration 30 nan\n",
      "cost after iteration 31 nan\n",
      "cost after iteration 32 nan\n",
      "cost after iteration 33 nan\n",
      "cost after iteration 34 nan\n",
      "cost after iteration 35 nan\n",
      "cost after iteration 36 nan\n",
      "cost after iteration 37 nan\n",
      "cost after iteration 38 nan\n",
      "cost after iteration 39 nan\n",
      "cost after iteration 40 nan\n",
      "cost after iteration 41 nan\n",
      "cost after iteration 42 nan\n",
      "cost after iteration 43 nan\n",
      "cost after iteration 44 nan\n",
      "cost after iteration 45 nan\n",
      "cost after iteration 46 nan\n",
      "cost after iteration 47 nan\n",
      "cost after iteration 48 nan\n",
      "cost after iteration 49 nan\n",
      "5.73099336187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/kernel/__main__.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/kernel/__main__.py:34: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "#gradient descent\n",
    "\n",
    "def gradient_descent(theta,X,y,alpha,num_iteration):\n",
    "    \n",
    "    ''' theta is a list of weights\n",
    "    X is 5000 x 400\n",
    "    y is 10 x 5000\n",
    "    '''\n",
    "    \n",
    "    final_theta=np.zeros_like(flatten_theta(theta))\n",
    "    min_cost=1000000\n",
    "    for i in range(num_iteration):\n",
    "        grad=backPropogation(theta,X,y_new)\n",
    "        a,b=unflatten_theta(grad)\n",
    "        theta=[theta[0]-alpha*a,theta[1]-alpha*b]\n",
    "        cost=nnCostFunction1(theta,X,y_new)\n",
    "        print \"cost after iteration\",str(i),cost\n",
    "        if cost<min_cost:\n",
    "            final_theta=theta\n",
    "            min_cost=cost\n",
    "    print min_cost\n",
    "    #return theta\n",
    " \n",
    "\n",
    "gradient_descent(temp_theta,X,y_new,0.1,50)\n",
    "\n",
    "def gradient_descent_momentum(theta,X,y,alpha,num_iteration,momentum):\n",
    "    \n",
    "    ''' theta is a list of weights\n",
    "    X is 5000 x 400\n",
    "    y is 10 x 5000\n",
    "    '''\n",
    "    \n",
    "    final_theta=np.zeros_like(flatten_theta(theta))\n",
    "    min_cost=1000000\n",
    "    \n",
    "    total_velocity_a=0\n",
    "    total_velocity_b=0\n",
    "    for i in range(num_iteration):\n",
    "        grad=backPropogation(theta,X,y_new)\n",
    "        a,b=unflatten_theta(grad)\n",
    "        \n",
    "        new_velocity_a,new_velocity_b=momentum*total_velocity_a-alpha*a,momentum*total_velocity_b-alpha*b\n",
    "        \n",
    "        total_velocity_a+=new_velocity_a\n",
    "        total_velocity_b+=new_velocity_b\n",
    "        \n",
    "        \n",
    "        \n",
    "        theta=[theta[0]+new_velocity_a,theta[1]+new_velocity_b]\n",
    "        cost=nnCostFunction1(theta,X,y_new)\n",
    "        print \"cost after iteration\",str(i),cost\n",
    "        if cost<min_cost:\n",
    "            final_theta=theta\n",
    "            min_cost=cost\n",
    "    print min_cost\n",
    "    #return theta\n",
    " \n",
    "gradient_descent_momentum(temp_theta,X,y_new,0.1,50,0.5)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "a1.shape (401, 5000)\n",
      "tehta1.shape (25, 401)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  6.18712766e-05,   9.38798109e-05,  -1.92593606e-04,\n",
       "        -1.68494568e-04,   3.48682758e-04,   2.30505234e-04,\n",
       "        -4.19602375e-05,   1.64744107e-05,   3.42491549e-04,\n",
       "        -2.09724069e-05])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import scipy.misc, scipy.io, scipy.optimize, scipy.special\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def recodeLabel( y, k ):\n",
    "\tm = shape(y)[0]\n",
    "\tout = zeros( ( k, m ) )\n",
    "\t\n",
    "\tfor i in range(0, m):\n",
    "\t\tout[y[i]-1, i] = 1\n",
    "\n",
    "\treturn out\n",
    "\n",
    "def mod( length, divisor ):\n",
    "\tdividend = array([x for x in range(1, length+1)])\n",
    "\tdivisor  = array([divisor for x in range(1, length+1)])\n",
    "\treturn mod( dividend, divisor ).reshape(1, length )\n",
    "\n",
    "def sigmoid( z ):\n",
    "\treturn scipy.special.expit(z)\n",
    "\n",
    "def sigmoidGradient( z ):\n",
    "\tsig = sigmoid(z)\n",
    "\treturn sig * (1 - sig)\n",
    "\n",
    "def predict( X, theta1, theta2 ):\n",
    "\ta1 = r_[ones((1, 1)), X.reshape( shape(X)[0], 1 )]\n",
    "\tz2 = sigmoid( theta1.dot( a1 ))\n",
    "\tz2 = r_[ones((1, 1)), z2]\n",
    "\tz3 = sigmoid(theta2.dot( z2 ))\n",
    "\treturn argmax(z3) + 1\n",
    "\n",
    "def paramUnroll( nn_params, input_layer_size, hidden_layer_size, num_labels ):\n",
    "\ttheta1_elems = ( input_layer_size + 1 ) * hidden_layer_size\n",
    "\ttheta1_size  = ( input_layer_size + 1, hidden_layer_size  )\n",
    "\ttheta2_size  = ( hidden_layer_size + 1, num_labels )\n",
    "\n",
    "\ttheta1 = nn_params[:theta1_elems].T.reshape( theta1_size ).T\t\n",
    "\ttheta2 = nn_params[theta1_elems:].T.reshape( theta2_size ).T\n",
    "\n",
    "\treturn (theta1, theta2)\n",
    "\n",
    "def feedForward( theta1, theta2, X, X_bias = None ):\n",
    "    one_rows = ones((1, shape(X)[0] ))\n",
    "\n",
    "    a1 = r_[one_rows, X.T]  if X_bias is None else X_bias\n",
    "    print 'a1.shape',a1.shape\n",
    "    print 'tehta1.shape',theta1.shape\n",
    "    z2 = theta1.dot( a1 )\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = r_[one_rows, a2] \n",
    "    z3 = theta2.dot( a2 )\n",
    "    a3 = sigmoid( z3 )\n",
    "\n",
    "    return (a1, a2, a3, z2, z3)\n",
    "\n",
    "def computeCost( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamda, yk = None, X_bias = None ):\n",
    "\tm, n \t\t\t\t= shape( X )\n",
    "\ttheta1, theta2 \t\t= paramUnroll( nn_params, input_layer_size, hidden_layer_size, num_labels )\n",
    "\ta1, a2, a3, z2, z3 \t= feedForward( theta1, theta2, X, X_bias )\n",
    "\n",
    "\t# calculating cost\n",
    "\tif yk is None:\n",
    "\t\tyk = recodeLabel( y, num_labels )\n",
    "\t\tassert shape(yk) == shape(a3), \"Error, shape of recoded y is different from a3\"\n",
    "\n",
    "\tterm1 \t\t= -yk * log( a3 )\n",
    "\tterm2 \t\t= (1 - yk) * log( 1 - a3 )\n",
    "\tleft_term \t= sum(term1 - term2) / m\n",
    "\tright_term \t= sum(theta1[:,1:] ** 2) + sum(theta2[:,1:] ** 2)\n",
    "\n",
    "\treturn left_term + right_term * lamda / (2 * m)\n",
    "\n",
    "def computeGradient( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamda, yk = None, X_bias = None ):\n",
    "    m, n \t\t\t\t= shape( X )\n",
    "    print X.shape\n",
    "    theta1, theta2 \t\t= paramUnroll( nn_params, input_layer_size, hidden_layer_size, num_labels )\n",
    "    a1, a2, a3, z2, z3 \t= feedForward( theta1, theta2, X, X_bias )\n",
    "\n",
    "    # back propagate\n",
    "    if yk is None:\n",
    "        yk = recodeLabel( y, num_labels )\n",
    "        assert shape(yk) == shape(a3), \"Error, shape of recoded y is different from a3\"\n",
    "    \n",
    "    \n",
    "    sigma3 = a3 - yk\n",
    "    sigma2 = theta2.T.dot( sigma3 ) * sigmoidGradient( r_[ones((1, m)), z2 ] )\n",
    "    sigma2 = sigma2[1:,:]\n",
    "    accum1 = sigma2.dot( a1.T ) / m\n",
    "    accum2 = sigma3.dot( a2.T ) / m\n",
    "\n",
    "    #accum1[:,1:] = accum1[:,1:] + (theta1[:,1:] * lamda / m)\n",
    "    #accum2[:,1:] = accum2[:,1:] + (theta2[:,1:] * lamda / m)\n",
    "    accum = array([accum1.T.reshape(-1).tolist() + accum2.T.reshape(-1).tolist()]).T\n",
    "    return ndarray.flatten(accum)\n",
    "\n",
    "def nnCostFunction( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamda ):\n",
    "\tm, n \t\t\t\t= shape( X )\n",
    "\ttheta1, theta2 \t\t= paramUnroll( nn_params, input_layer_size, hidden_layer_size, num_labels )\n",
    "\ta1, a2, a3, z2, z3 \t= feedForward( theta1, theta2, X )\n",
    "\n",
    "\t# calculating cost\n",
    "\tyk \t\t\t= recodeLabel( y.T, num_labels )\n",
    "\tassert shape(yk) == shape(a3), \"Error, shape of recoded y is different from a3\"\n",
    "\n",
    "\tterm1 \t\t= -yk * log( a3 )\n",
    "\tterm2 \t\t= (1-yk) * log( 1 - a3 )\n",
    "\tleft_term \t= sum(term1 - term2) / m\n",
    "\tright_term \t= sum(theta1[:,1:] ** 2) + sum(theta2[:,1:] ** 2)\n",
    "\tright_term \t= right_term * lamda / (2 * m)\n",
    "\tcost \t\t= left_term + right_term\n",
    "\n",
    "\t# back propagate\n",
    "\tsigma3 = a3 - yk\n",
    "\tsigma2 = theta2.T.dot( sigma3 ) * sigmoidGradient( r_[ ones((1, m)), z2 ] )\n",
    "\tsigma2 = sigma2[1:,:]\n",
    "\n",
    "\taccum1 = sigma2.dot( a1.T ) / m\n",
    "\taccum2 = sigma3.dot( a2.T ) / m\n",
    "\n",
    "\taccum1[:,1:] = accum1[:,1:] + (theta1[:,1:] * lamda / m)\n",
    "\taccum2[:,1:] = accum2[:,1:] + (theta2[:,1:] * lamda / m)\n",
    "\n",
    "\tgradient \t = array([accum1.T.reshape(-1).tolist() + accum2.T.reshape(-1).tolist()]).T\n",
    "\n",
    "\treturn (cost, gradient)\n",
    "\n",
    "def randInitializeWeights(L_in, L_out):\n",
    "\te = 0.12\n",
    "\tw = random.random((L_out, L_in + 1)) * 2 * e - e\n",
    "\treturn w\n",
    "\n",
    "def debugInitializeWeights(fan_out, fan_in):\n",
    "\tnum_elements = fan_out * (1+fan_in)\n",
    "\tw = array([sin(x) / 10 for x in range(1, num_elements+1)])\n",
    "\tw = w.reshape( 1+fan_in, fan_out ).T\n",
    "\treturn w\n",
    "\n",
    "\n",
    "def computeNumericalGradient( theta, input_layer_size, hidden_layer_size, num_labels, X, y, lamda ):\n",
    "\tnumgrad \t= zeros( shape(theta) )\n",
    "\tperturb \t= zeros( shape(theta) ) #38 x 1\n",
    "\te = 1e-4\n",
    "\n",
    "\tnum_elements = shape(theta)[0]\n",
    "\tyk = recodeLabel( y, num_labels )\n",
    "\n",
    "\tfor p in range(0, num_elements) :\n",
    "\t\tperturb[p] = e\n",
    "\t\tloss1 = computeCost( theta - perturb, input_layer_size, hidden_layer_size, num_labels, X, y, lamda, yk )\n",
    "\t\tloss2 = computeCost( theta + perturb, input_layer_size, hidden_layer_size, num_labels, X, y, lamda, yk )\n",
    "\t\tnumgrad[p] = (loss2 - loss1) / (2 * e)\n",
    "\t\tperturb[p] = 0\n",
    "\n",
    "\treturn numgrad\n",
    "\n",
    "def checkNNGradients( lamda = 0.0 ):\n",
    "\tinput_layer_size \t= 3\n",
    "\thidden_layer_size \t= 5\n",
    "\tnum_labels \t\t\t= 3\n",
    "\tm \t\t\t\t\t= 5\n",
    "\n",
    "\ttheta1 = debugInitializeWeights( hidden_layer_size, input_layer_size )\n",
    "\ttheta2 = debugInitializeWeights( num_labels, hidden_layer_size )\n",
    "\n",
    "\tX = debugInitializeWeights( m, input_layer_size - 1 )\n",
    "\ty = 1 + mod( m, num_labels )\n",
    "\n",
    "\tnn_params \t= array([theta1.T.reshape(-1).tolist() + theta2.T.reshape(-1).tolist()]).T\n",
    "\tgradient \t= nnCostFunction( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamda )[1]\n",
    "\tnumgrad \t= computeNumericalGradient( nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamda )\n",
    "\tdiff = linalg.norm( numgrad - gradient ) / (linalg.norm( numgrad + gradient ))\n",
    "\tprint diff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def part1_1():\n",
    "\tmat = scipy.io.loadmat( \"/Users/saburookita/Downloads/mlclass-ex4-004/mlclass-ex4/ex4data1.mat\" )\n",
    "\tX, y = mat['X'], mat['y']\n",
    "\tdisplayData( X )\n",
    "\n",
    "def part1_2():\n",
    "\tmat = scipy.io.loadmat( \"/Users/saburookita/Downloads/mlclass-ex4-004/mlclass-ex4/ex4data1.mat\" )\n",
    "\tX, y = mat['X'], mat['y']\n",
    "\tm, n = shape(X)\n",
    "\n",
    "\t# Load the weights\n",
    "\tmat = scipy.io.loadmat( \"/Users/saburookita/Downloads/mlclass-ex4-004/mlclass-ex4/ex4weights.mat\" )\n",
    "\ttheta1, theta2 = mat['Theta1'], mat['Theta2']\n",
    "\n",
    "\tnn_params = [theta1.T.flatten(), theta2.T.flatten()]\n",
    "\n",
    "def part1_3():\n",
    "\tmat = scipy.io.loadmat( \"ex4data1.mat\" )\n",
    "\tX, y = mat['X'], mat['y']\n",
    "\n",
    "\t# Load the weights\n",
    "\tmat = scipy.io.loadmat( \"ex4weights.mat\" )\n",
    "\ttheta1, theta2 = mat['Theta1'], mat['Theta2']\n",
    "\n",
    "\tinput_layer_size  \t= 400\n",
    "\thidden_layer_size \t= 25\n",
    "\tnum_labels \t\t\t= 10\n",
    "\tlamda \t\t\t\t= 0\n",
    "\n",
    "\tparams = r_[theta1.T.flatten(), theta2.T.flatten()]\n",
    "\n",
    "\treturn computeGradient( params, input_layer_size, hidden_layer_size, num_labels, X, y, lamda )\n",
    "\t#print computeCost( params, input_layer_size, hidden_layer_size, num_labels, X, y, lamda )\n",
    "\n",
    "\n",
    "def main():\n",
    "\tset_printoptions(precision=6, linewidth=200)\n",
    "\tpart1_1()\n",
    "\tpart1_2()\n",
    "\tpart1_3()\n",
    "\tpart1_4()\n",
    "\tpart2_1()\n",
    "\tpart2_2()\n",
    "\tpart2_3()\n",
    "\tpart2_5()\n",
    "\tpart2_6()\n",
    "\t\n",
    "\n",
    "a=part1_3()\n",
    "\n",
    "a[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.18712766e-05,   9.38798109e-05,  -1.92593606e-04,\n",
       "        -1.68494568e-04,   3.48682758e-04,   2.30505234e-04,\n",
       "        -4.19602375e-05,   1.64744107e-05,   3.42491549e-04,\n",
       "        -2.09724069e-05])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
